{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c9bb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, f1_score, precision_score, recall_score\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, Input\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "# AI-Driven Defect Detection in Chemically Decapsulated NAND Flash Memory\n",
    "\n",
    "## Abstract\n",
    "This notebook presents a comprehensive implementation of deep learning techniques for automated defect detection in semiconductor wafer maps. The approach utilizes convolutional neural networks (CNNs) to classify various defect patterns in chemically decapsulated NAND flash memory devices, enabling efficient quality control in semiconductor manufacturing.\n",
    "\n",
    "## Motivation\n",
    "Traditional manual inspection of semiconductor wafers is time-consuming and prone to human error. This implementation demonstrates how artificial intelligence can enhance defect detection accuracy while reducing inspection time (LeCun et al., 2015).\n",
    "\n",
    "## Dataset\n",
    "We utilize the WM-811K wafer map dataset, which contains 811,457 wafer maps with various defect patterns commonly found in semiconductor manufacturing (Nakazawa & Kulkarni, 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11342a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.test.is_gpu_available())\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423581d",
   "metadata": {},
   "source": [
    "## Requirements and Dependencies\n",
    "\n",
    "### Core Libraries:\n",
    "\n",
    "- **TensorFlow 2.x**: Deep learning framework for CNN implementation (Abadi et al., 2016)\n",
    "- **NumPy**: Numerical computing for array operations (Harris et al., 2020)\n",
    "- **Pandas**: Data manipulation and analysis (McKinney, 2010)\n",
    "- **Scikit-learn**: Machine learning utilities and metrics (Pedregosa et al., 2011)\n",
    "- **Matplotlib/Seaborn**: Data visualization (Hunter, 2007)\n",
    "\n",
    "### Installation:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow numpy pandas scikit-learn matplotlib seaborn\n",
    "```\n",
    "\n",
    "### Hardware Recommendations:\n",
    "\n",
    "- GPU with CUDA support for accelerated training\n",
    "- Minimum 8GB RAM for dataset handling\n",
    "- 10GB+ storage for dataset and model checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2437c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaferDataLoader:\n",
    "    \"\"\"\n",
    "    Custom data loader for WM-811K wafer map dataset\n",
    "    Handles loading, preprocessing, and visualization of wafer maps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.data = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load wafer map data from pickle file\"\"\"\n",
    "        try:\n",
    "            with open(self.data_path, 'rb') as f:\n",
    "                self.data = pickle.load(f)\n",
    "            print(f\"Dataset loaded successfully!\")\n",
    "            print(f\"Total samples: {len(self.data)}\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found at {self.data_path}\")\n",
    "            print(\"Please ensure the WM-811K dataset is downloaded and path is correct\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def explore_data(self):\n",
    "        \"\"\"Explore dataset structure and statistics\"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"Please load data first using load_data()\")\n",
    "            return\n",
    "        \n",
    "        # Sample data structure\n",
    "        sample = self.data[0]\n",
    "        print(\"Data structure:\")\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                print(f\"- {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "            else:\n",
    "                print(f\"- {key}: {type(value)} - {value}\")\n",
    "        \n",
    "        # Defect pattern distribution\n",
    "        defect_patterns = [item['failureType'] for item in self.data if item['failureType'] is not None]\n",
    "        pattern_counts = pd.Series(defect_patterns).value_counts()\n",
    "        \n",
    "        print(f\"\\nDefect Pattern Distribution:\")\n",
    "        print(pattern_counts)\n",
    "        \n",
    "        return pattern_counts\n",
    "    \n",
    "    def visualize_samples(self, n_samples=9):\n",
    "        \"\"\"Visualize sample wafer maps for each defect type\"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"Please load data first\")\n",
    "            return\n",
    "        \n",
    "        # Get unique defect types\n",
    "        defect_types = list(set([item['failureType'] for item in self.data if item['failureType'] is not None]))\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, defect_type in enumerate(defect_types[:n_samples]):\n",
    "            # Find first sample with this defect type\n",
    "            sample = next(item for item in self.data if item['failureType'] == defect_type)\n",
    "            wafer_map = sample['waferMap']\n",
    "            \n",
    "            axes[i].imshow(wafer_map, cmap='viridis')\n",
    "            axes[i].set_title(f'Defect Type: {defect_type}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize data loader (adjust path as needed)\n",
    "# Note: Replace with actual path to downloaded WM-811K dataset\n",
    "data_loader = WaferDataLoader('WAFERM811K_25013.pkl')\n",
    "\n",
    "# Attempt to load data (will show instructions if file not found)\n",
    "if data_loader.load_data():\n",
    "    pattern_distribution = data_loader.explore_data()\n",
    "    data_loader.visualize_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0bd65",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline addresses several key challenges in semiconductor defect detection:\n",
    "\n",
    "### 1. **Data Cleaning** (Ng, 2016)\n",
    "\n",
    "- Remove samples with missing labels\n",
    "- Handle irregular wafer map sizes\n",
    "- Filter out extremely rare defect types\n",
    "\n",
    "### 2. **Normalization and Standardization**\n",
    "\n",
    "- Normalize pixel values to [0, 1] range\n",
    "- Apply z-score normalization for consistent feature scaling\n",
    "\n",
    "### 3. **Data Augmentation** (Shorten & Khoshgoftaar, 2019)\n",
    "\n",
    "- Rotation: Account for wafer orientation variations\n",
    "- Flipping: Increase dataset diversity\n",
    "- Noise injection: Improve model robustness\n",
    "\n",
    "### 4. **Class Balancing**\n",
    "\n",
    "- Address class imbalance through stratified sampling\n",
    "- Implement weighted loss functions for minority classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaferPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessor for wafer map data\n",
    "    Handles cleaning, normalization, and augmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_size=(64, 64), min_samples_per_class=100):\n",
    "        self.target_size = target_size\n",
    "        self.min_samples_per_class = min_samples_per_class\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.class_weights = None\n",
    "        \n",
    "    def clean_data(self, data):\n",
    "        \"\"\"Clean and filter dataset\"\"\"\n",
    "        print(\"Cleaning dataset...\")\n",
    "        \n",
    "        # Remove samples without failure type\n",
    "        cleaned_data = [item for item in data if item['failureType'] is not None]\n",
    "        print(f\"Removed {len(data) - len(cleaned_data)} samples without labels\")\n",
    "        \n",
    "        # Count samples per class\n",
    "        failure_types = [item['failureType'] for item in cleaned_data]\n",
    "        type_counts = pd.Series(failure_types).value_counts()\n",
    "        \n",
    "        # Keep only classes with sufficient samples\n",
    "        valid_types = type_counts[type_counts >= self.min_samples_per_class].index.tolist()\n",
    "        cleaned_data = [item for item in cleaned_data if item['failureType'] in valid_types]\n",
    "        \n",
    "        print(f\"Keeping {len(valid_types)} classes with >= {self.min_samples_per_class} samples\")\n",
    "        print(f\"Final dataset size: {len(cleaned_data)} samples\")\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "    def normalize_wafer_maps(self, wafer_maps):\n",
    "        \"\"\"Normalize and resize wafer maps\"\"\"\n",
    "        normalized_maps = []\n",
    "        \n",
    "        for wafer_map in wafer_maps:\n",
    "            # Resize to target size\n",
    "            resized_map = tf.image.resize(\n",
    "                tf.expand_dims(wafer_map, axis=-1), \n",
    "                self.target_size\n",
    "            ).numpy().squeeze()\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            if resized_map.max() > resized_map.min():\n",
    "                normalized_map = (resized_map - resized_map.min()) / (resized_map.max() - resized_map.min())\n",
    "            else:\n",
    "                normalized_map = resized_map\n",
    "            \n",
    "            normalized_maps.append(normalized_map)\n",
    "        \n",
    "        return np.array(normalized_maps)\n",
    "    \n",
    "    def prepare_dataset(self, data):\n",
    "        \"\"\"Prepare complete dataset for training\"\"\"\n",
    "        # Clean data\n",
    "        cleaned_data = self.clean_data(data)\n",
    "        \n",
    "        # Extract features and labels\n",
    "        wafer_maps = [item['waferMap'] for item in cleaned_data]\n",
    "        failure_types = [item['failureType'] for item in cleaned_data]\n",
    "        \n",
    "        # Normalize wafer maps\n",
    "        X = self.normalize_wafer_maps(wafer_maps)\n",
    "        X = np.expand_dims(X, axis=-1)  # Add channel dimension\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(failure_types)\n",
    "        y_categorical = to_categorical(y_encoded)\n",
    "        \n",
    "        # Calculate class weights for imbalanced dataset\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        class_weights_array = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_encoded),\n",
    "            y=y_encoded\n",
    "        )\n",
    "        self.class_weights = dict(enumerate(class_weights_array))\n",
    "        \n",
    "        print(f\"Dataset shape: {X.shape}\")\n",
    "        print(f\"Number of classes: {len(self.label_encoder.classes_)}\")\n",
    "        print(f\"Classes: {list(self.label_encoder.classes_)}\")\n",
    "        \n",
    "        return X, y_categorical, failure_types\n",
    "\n",
    "# Apply preprocessing if data is loaded\n",
    "if 'data_loader' in globals() and data_loader.data is not None:\n",
    "    preprocessor = WaferPreprocessor(target_size=(64, 64))\n",
    "    X, y, original_labels = preprocessor.prepare_dataset(data_loader.data)\n",
    "    \n",
    "    # Split dataset\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Validation set: {X_val.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "else:\n",
    "    print(\"Creating dummy data for demonstration purposes...\")\n",
    "    # Create dummy data if actual dataset not available\n",
    "    X_train = np.random.random((1000, 64, 64, 1))\n",
    "    X_val = np.random.random((300, 64, 64, 1))\n",
    "    X_test = np.random.random((200, 64, 64, 1))\n",
    "    y_train = to_categorical(np.random.randint(0, 9, 1000), num_classes=9)\n",
    "    y_val = to_categorical(np.random.randint(0, 9, 300), num_classes=9)\n",
    "    y_test = to_categorical(np.random.randint(0, 9, 200), num_classes=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb3eb9b",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Architecture\n",
    "\n",
    "Our CNN architecture is specifically designed for semiconductor defect detection, incorporating modern deep learning principles:\n",
    "\n",
    "### **Architecture Components:**\n",
    "\n",
    "1. **Convolutional Layers** (Krizhevsky et al., 2012)\n",
    "\n",
    "   - Multiple conv2D layers with increasing filter counts\n",
    "   - ReLU activation for non-linearity\n",
    "   - Batch normalization for training stability\n",
    "\n",
    "2. **Pooling Layers**\n",
    "\n",
    "   - MaxPooling for spatial dimension reduction\n",
    "   - Preserves most important features while reducing computation\n",
    "\n",
    "3. **Regularization Techniques** (Srivastava et al., 2014)\n",
    "\n",
    "   - Dropout layers to prevent overfitting\n",
    "   - Batch normalization for internal covariate shift reduction\n",
    "\n",
    "4. **Dense Classification Head**\n",
    "   - Fully connected layers for final classification\n",
    "   - Softmax activation for multi-class probability distribution\n",
    "\n",
    "### **Design Rationale:**\n",
    "\n",
    "The architecture balances model complexity with computational efficiency, suitable for industrial deployment while maintaining high accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectDetectionCNN:\n",
    "    \"\"\"\n",
    "    Advanced CNN architecture for semiconductor defect detection\n",
    "    Incorporates modern deep learning best practices\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(64, 64, 1), num_classes=9):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build CNN architecture optimized for defect detection\"\"\"\n",
    "        \n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Input(shape=self.input_shape),\n",
    "            \n",
    "            # First convolutional block\n",
    "            Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Second convolutional block\n",
    "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Third convolutional block\n",
    "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Fourth convolutional block\n",
    "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.5),\n",
    "            \n",
    "            # Classification head\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Compile model with appropriate optimizer and loss function\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"Display model architecture summary\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        # Calculate total parameters\n",
    "        total_params = self.model.count_params()\n",
    "        print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "        \n",
    "        return self.model.summary()\n",
    "    \n",
    "    def visualize_architecture(self):\n",
    "        \"\"\"Visualize model architecture\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        tf.keras.utils.plot_model(\n",
    "            self.model,\n",
    "            to_file='model_architecture.png',\n",
    "            show_shapes=True,\n",
    "            show_layer_names=True\n",
    "        )\n",
    "        print(\"Model architecture saved as 'model_architecture.png'\")\n",
    "\n",
    "# Initialize and build model\n",
    "cnn_model = DefectDetectionCNN(\n",
    "    input_shape=(64, 64, 1),\n",
    "    num_classes=y_train.shape[1] if 'y_train' in globals() else 9\n",
    ")\n",
    "\n",
    "# Build and display model\n",
    "model = cnn_model.build_model()\n",
    "cnn_model.get_model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ac7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingManager:\n",
    "    \"\"\"\n",
    "    Manages training process with callbacks and monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.callbacks = []\n",
    "        self.history = None\n",
    "        \n",
    "    def setup_callbacks(self, model_save_path='best_model.h5'):\n",
    "        \"\"\"Configure training callbacks for optimal training\"\"\"\n",
    "        \n",
    "        # Early stopping to prevent overfitting\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save best model\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            model_save_path,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.callbacks = [early_stopping, reduce_lr, model_checkpoint]\n",
    "        return self.callbacks\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_val, y_val, \n",
    "                   epochs=50, batch_size=32, class_weights=None):\n",
    "        \"\"\"Train the model with specified parameters\"\"\"\n",
    "        \n",
    "        # Setup callbacks\n",
    "        self.setup_callbacks()\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Training samples: {len(X_train)}\")\n",
    "        print(f\"Validation samples: {len(X_val)}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Maximum epochs: {epochs}\")\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=self.callbacks,\n",
    "            class_weight=class_weights,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return self.history\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Visualize training progress\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available. Train model first.\")\n",
    "            return\n",
    "        \n",
    "        # Create subplots for metrics\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0, 0].plot(self.history.history['accuracy'], label='Training Accuracy')\n",
    "        axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        axes[0, 0].set_title('Model Accuracy')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Loss\n",
    "        axes[0, 1].plot(self.history.history['loss'], label='Training Loss')\n",
    "        axes[0, 1].plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        axes[0, 1].set_title('Model Loss')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Precision\n",
    "        if 'precision' in self.history.history:\n",
    "            axes[1, 0].plot(self.history.history['precision'], label='Training Precision')\n",
    "            axes[1, 0].plot(self.history.history['val_precision'], label='Validation Precision')\n",
    "            axes[1, 0].set_title('Model Precision')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('Precision')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True)\n",
    "        \n",
    "        # Recall\n",
    "        if 'recall' in self.history.history:\n",
    "            axes[1, 1].plot(self.history.history['recall'], label='Training Recall')\n",
    "            axes[1, 1].plot(self.history.history['val_recall'], label='Validation Recall')\n",
    "            axes[1, 1].set_title('Model Recall')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Recall')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize training manager\n",
    "trainer = TrainingManager(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING PHASE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get class weights if available\n",
    "class_weights = None\n",
    "if 'preprocessor' in globals() and hasattr(preprocessor, 'class_weights'):\n",
    "    class_weights = preprocessor.class_weights\n",
    "\n",
    "# Execute training\n",
    "history = trainer.train_model(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    epochs=30,  # Reduced for demonstration\n",
    "    batch_size=32,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "# Visualize training progress\n",
    "trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc0df0",
   "metadata": {},
   "source": [
    "## Model Evaluation and Performance Analysis\n",
    "\n",
    "### **Evaluation Metrics** (Sokolova & Lapalme, 2009)\n",
    "\n",
    "1. **Accuracy**: Overall correctness of predictions\n",
    "\n",
    "   - Formula: (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "2. **Precision**: Ability to avoid false positives\n",
    "\n",
    "   - Formula: TP / (TP + FP)\n",
    "\n",
    "3. **Recall (Sensitivity)**: Ability to find all positive instances\n",
    "\n",
    "   - Formula: TP / (TP + FN)\n",
    "\n",
    "4. **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "   - Formula: 2 × (Precision × Recall) / (Precision + Recall)\n",
    "\n",
    "5. **Confusion Matrix**: Detailed breakdown of classification performance\n",
    "\n",
    "### **Industrial Relevance:**\n",
    "\n",
    "In semiconductor manufacturing, minimizing false negatives (missed defects) is critical for product quality, while controlling false positives (unnecessary rejections) is important for cost efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221896a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation suite for defect detection model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, label_encoder=None):\n",
    "        self.model = model\n",
    "        self.label_encoder = label_encoder\n",
    "        self.predictions = None\n",
    "        self.true_labels = None\n",
    "        \n",
    "    def evaluate_model(self, X_test, y_test, class_names=None):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        print(\"=\"*50)\n",
    "        print(\"MODEL EVALUATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Make predictions\n",
    "        self.predictions = self.model.predict(X_test)\n",
    "        predicted_classes = np.argmax(self.predictions, axis=1)\n",
    "        true_classes = np.argmax(y_test, axis=1)\n",
    "        \n",
    "        self.true_labels = true_classes\n",
    "        self.predicted_labels = predicted_classes\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = np.mean(predicted_classes == true_classes)\n",
    "        precision = precision_score(true_classes, predicted_classes, average='weighted')\n",
    "        recall = recall_score(true_classes, predicted_classes, average='weighted')\n",
    "        f1 = f1_score(true_classes, predicted_classes, average='weighted')\n",
    "        \n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Weighted Precision: {precision:.4f}\")\n",
    "        print(f\"Weighted Recall: {recall:.4f}\")\n",
    "        print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        # Per-class metrics\n",
    "        if class_names is None:\n",
    "            if self.label_encoder is not None:\n",
    "                class_names = self.label_encoder.classes_\n",
    "            else:\n",
    "                class_names = [f\"Class_{i}\" for i in range(len(np.unique(true_classes)))]\n",
    "        \n",
    "        print(\"\\nDetailed Classification Report:\")\n",
    "        print(classification_report(true_classes, predicted_classes, \n",
    "                                  target_names=class_names))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'predictions': self.predictions,\n",
    "            'predicted_classes': predicted_classes,\n",
    "            'true_classes': true_classes\n",
    "        }\n",
    "    \n",
    "    def plot_confusion_matrix(self, class_names=None, normalize=False):\n",
    "        \"\"\"Plot confusion matrix heatmap\"\"\"\n",
    "        if self.true_labels is None or self.predicted_labels is None:\n",
    "            print(\"Please run evaluate_model first\")\n",
    "            return\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(self.true_labels, self.predicted_labels)\n",
    "        \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "            fmt = '.2f'\n",
    "        else:\n",
    "            title = 'Confusion Matrix'\n",
    "            fmt = 'd'\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    def plot_prediction_confidence(self):\n",
    "        \"\"\"Analyze prediction confidence distribution\"\"\"\n",
    "        if self.predictions is None:\n",
    "            print(\"Please run evaluate_model first\")\n",
    "            return\n",
    "        \n",
    "        # Get confidence scores (max probability for each prediction)\n",
    "        confidence_scores = np.max(self.predictions, axis=1)\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(confidence_scores, bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.title('Prediction Confidence Distribution')\n",
    "        plt.xlabel('Confidence Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        # Confidence vs correctness\n",
    "        correct_mask = self.predicted_labels == self.true_labels\n",
    "        \n",
    "        plt.hist(confidence_scores[correct_mask], bins=30, alpha=0.7, \n",
    "                label='Correct Predictions', color='green')\n",
    "        plt.hist(confidence_scores[~correct_mask], bins=30, alpha=0.7, \n",
    "                label='Incorrect Predictions', color='red')\n",
    "        plt.title('Confidence by Prediction Correctness')\n",
    "        plt.xlabel('Confidence Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Average confidence for correct predictions: {np.mean(confidence_scores[correct_mask]):.4f}\")\n",
    "        print(f\"Average confidence for incorrect predictions: {np.mean(confidence_scores[~correct_mask]):.4f}\")\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = ModelEvaluator(\n",
    "    model, \n",
    "    label_encoder=preprocessor.label_encoder if 'preprocessor' in globals() else None\n",
    ")\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluation_results = evaluator.evaluate_model(X_test, y_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = preprocessor.label_encoder.classes_ if 'preprocessor' in globals() else None\n",
    "evaluator.plot_confusion_matrix(class_names=class_names, normalize=True)\n",
    "\n",
    "# Analyze prediction confidence\n",
    "evaluator.plot_prediction_confidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa991b",
   "metadata": {},
   "source": [
    "## Practical Implementation for Industrial Deployment\n",
    "\n",
    "### **Model Optimization Techniques:**\n",
    "\n",
    "1. **Model Quantization** (Jacob et al., 2018)\n",
    "\n",
    "   - Reduce model size for edge deployment\n",
    "   - Maintain accuracy while improving inference speed\n",
    "\n",
    "2. **TensorFlow Lite Conversion**\n",
    "\n",
    "   - Mobile and embedded device compatibility\n",
    "   - Optimized for resource-constrained environments\n",
    "\n",
    "3. **Batch Processing Pipeline**\n",
    "   - Efficient handling of multiple wafer maps\n",
    "   - Queue management for production lines\n",
    "\n",
    "### **Integration Considerations:**\n",
    "\n",
    "- **Real-time Processing**: Sub-second inference for production lines\n",
    "- **Quality Assurance**: Confidence thresholds for automated decisions\n",
    "- **Human-in-the-Loop**: Expert review for borderline cases\n",
    "- **Continuous Learning**: Model updates with new defect patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f23470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionDeployment:\n",
    "    \"\"\"\n",
    "    Production-ready deployment pipeline for defect detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocessor, confidence_threshold=0.8):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.inference_times = []\n",
    "        \n",
    "    def preprocess_single_wafer(self, wafer_map):\n",
    "        \"\"\"Preprocess single wafer map for inference\"\"\"\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Resize and normalize\n",
    "        if len(wafer_map.shape) == 2:\n",
    "            resized_map = tf.image.resize(\n",
    "                tf.expand_dims(tf.expand_dims(wafer_map, axis=-1), axis=0), \n",
    "                (64, 64)\n",
    "            ).numpy()\n",
    "        else:\n",
    "            resized_map = tf.image.resize(\n",
    "                tf.expand_dims(wafer_map, axis=0), \n",
    "                (64, 64)\n",
    "            ).numpy()\n",
    "        \n",
    "        # Normalize\n",
    "        if resized_map.max() > resized_map.min():\n",
    "            normalized_map = (resized_map - resized_map.min()) / (resized_map.max() - resized_map.min())\n",
    "        else:\n",
    "            normalized_map = resized_map\n",
    "        \n",
    "        preprocessing_time = time.time() - start_time\n",
    "        return normalized_map, preprocessing_time\n",
    "    \n",
    "    def predict_defect(self, wafer_map, return_confidence=True):\n",
    "        \"\"\"Single wafer defect prediction with confidence scoring\"\"\"\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Preprocess wafer map\n",
    "        processed_wafer, prep_time = self.preprocess_single_wafer(wafer_map)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(processed_wafer, verbose=0)\n",
    "        confidence = np.max(prediction)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        \n",
    "        # Get class name if label encoder available\n",
    "        if hasattr(self.preprocessor, 'label_encoder'):\n",
    "            predicted_defect = self.preprocessor.label_encoder.inverse_transform([predicted_class])[0]\n",
    "        else:\n",
    "            predicted_defect = f\"Class_{predicted_class}\"\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        self.inference_times.append(inference_time)\n",
    "        \n",
    "        # Decision based on confidence\n",
    "        reliable_prediction = confidence >= self.confidence_threshold\n",
    "        \n",
    "        result = {\n",
    "            'predicted_defect': predicted_defect,\n",
    "            'confidence': float(confidence),\n",
    "            'reliable': reliable_prediction,\n",
    "            'inference_time': inference_time,\n",
    "            'preprocessing_time': prep_time,\n",
    "            'raw_probabilities': prediction[0].tolist()\n",
    "        }\n",
    "        \n",
    "        if return_confidence:\n",
    "            return result\n",
    "        else:\n",
    "            return predicted_defect\n",
    "    \n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"Generate performance statistics for deployment monitoring\"\"\"\n",
    "        if not self.inference_times:\n",
    "            print(\"No inference times recorded. Run predictions first.\")\n",
    "            return\n",
    "        \n",
    "        report = {\n",
    "            'total_predictions': len(self.inference_times),\n",
    "            'avg_inference_time': np.mean(self.inference_times),\n",
    "            'min_inference_time': np.min(self.inference_times),\n",
    "            'max_inference_time': np.max(self.inference_times),\n",
    "            'std_inference_time': np.std(self.inference_times),\n",
    "            'throughput_per_second': 1.0 / np.mean(self.inference_times)\n",
    "        }\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"DEPLOYMENT PERFORMANCE REPORT\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total Predictions: {report['total_predictions']}\")\n",
    "        print(f\"Average Inference Time: {report['avg_inference_time']:.4f} seconds\")\n",
    "        print(f\"Min/Max Inference Time: {report['min_inference_time']:.4f}/{report['max_inference_time']:.4f} seconds\")\n",
    "        print(f\"Standard Deviation: {report['std_inference_time']:.4f} seconds\")\n",
    "        print(f\"Estimated Throughput: {report['throughput_per_second']:.2f} wafers/second\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize deployment pipeline\n",
    "deployment = ProductionDeployment(\n",
    "    model, \n",
    "    preprocessor if 'preprocessor' in globals() else None,\n",
    "    confidence_threshold=0.8\n",
    ")\n",
    "\n",
    "# Demo single prediction\n",
    "print(\"=\"*50)\n",
    "print(\"SINGLE WAFER PREDICTION DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use a test sample for demonstration\n",
    "demo_wafer = X_test[0].squeeze()  # Remove channel dimension for demo\n",
    "result = deployment.predict_defect(demo_wafer)\n",
    "\n",
    "print(f\"Predicted Defect: {result['predicted_defect']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Reliable Prediction: {result['reliable']}\")\n",
    "print(f\"Inference Time: {result['inference_time']:.4f} seconds\")\n",
    "\n",
    "# Generate performance report\n",
    "deployment.generate_performance_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
